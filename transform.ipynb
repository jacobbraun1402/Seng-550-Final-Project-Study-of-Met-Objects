{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql.functions import col, udf, when, filter\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, Word2Vec, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "# import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to experiment with transforming the extracted data and building logistic regression models off of it, as well as evaluating those models. The predictionPipeline file consolodates this into one script that could be used to make predictions on an arbitrary extracted dataset, using the best model found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 20:56:23 WARN Utils: Your hostname, Jacobs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "24/12/19 20:56:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/19 20:56:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"Transform\").config(\"spark.ui.port\", '4050').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get csv file that was created in the extract file\n",
    "# extract program is set up so that only one csv will be in the extractedDataset folder, so we can use this regex to\n",
    "# get the right file without having to type the whole name out\n",
    "fileName = subprocess.check_output('ls extractedDataset | grep \".*.csv\"', shell=True, text=True).removesuffix('\\n')\n",
    "path = os.getcwd() + \"/extractedDataset/\" + fileName\n",
    "df = spark.read.csv(path, header=True, nullValue='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that applies all the transformations needed to the dataset to perform model training\n",
    "def transformDataset(df):\n",
    "    # in the database, they only store a value in the gender field if the artist is female\n",
    "    # otherwise it is null\n",
    "    # if there are multiple artists attributed to an object, the gender of each artist is shown, separated by a '|'\n",
    "    df = genderColumn(df)\n",
    "\n",
    "    # drop culture because it has a ton of nulls, I think artist nationality along with the department it's from and year\n",
    "    # it was made will still allow you to somewhat infer the culture it came from\n",
    "    df = df.drop('culture')\n",
    "    # can't have null values in model\n",
    "    df = df.dropna()\n",
    "\n",
    "    # change nationality column into word2Vec encoding\n",
    "    df = vectorizeNationality(df)\n",
    "\n",
    "    # change object name column into word2vec encoding\n",
    "    df = vectorizeObject(df)\n",
    "\n",
    "    #change department to one hot encoding, since this is categorical\n",
    "    df = oneHotDepartment(df)\n",
    "\n",
    "    #change isHighlight and isTimelineWork to integers\n",
    "    df = df.withColumn('isHighlightInt', when(df.isHighlight == True, 1).otherwise(0)).drop('isHighlight')\n",
    "    df = df.withColumn('isTimelineWorkInt', when(df.isTimelineWork == True, 1).otherwise(0)).drop('isTimelineWork')\n",
    "\n",
    "    # cast accession year and object end date data types to integers\n",
    "\n",
    "    df = df.withColumn('accessionYearInt', df.accessionYear.cast(IntegerType())).drop('accessionYear')\\\n",
    "         .withColumn('objectDate', df.objectEndDate.cast(IntegerType())).drop('objectEndDate')\\\n",
    "          .drop('objectID')\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom function to convert gender column to boolean indicating if artist is female\n",
    "# 0=Male, 1=Female\n",
    "# in the database, they only store a value in the gender field if the artist is female\n",
    "# otherwise it is null\n",
    "def convertGender(s):\n",
    "    if s == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# register user defined functions\n",
    "convertGenderUDF = udf(lambda x:convertGender(x), IntegerType())\n",
    "\n",
    "# take artistGender column, apply user-defined function to columns and put it in new column \n",
    "def genderColumn(df):\n",
    "    # change nulls to male here because I don't know what datatype spark uses to represent nulls\n",
    "    # So I cant detect if a value is null when I'm applying convertGender to each individual value in the column\n",
    "    x = df.na.fill(value='male', subset=['artistGender'])\n",
    "    # create new column that is made from applying the custom function to the gender column\n",
    "    x = x.withColumn(\"isFemale\", convertGenderUDF(col(\"artistGender\")))\n",
    "    x = x.drop(\"artistGender\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeNationality(df):\n",
    "    # change strings in nationality column to list of tokens, which is just each word in the string\n",
    "    nationalityTokenizer = Tokenizer(outputCol=\"nationalityWords\")\n",
    "    nationalityTokenizer.setInputCol(\"artistNationality\")\n",
    "    df = nationalityTokenizer.transform(df)\n",
    "\n",
    "    # word2vec doesn't really extract any interesting patterns from the nationality column\n",
    "    # but it does help in reducing its dimensions\n",
    "    # in the bag of words approach, there were 108 dimensions\n",
    "    word2vec = Word2Vec(vectorSize=5, inputCol=\"nationalityWords\", outputCol='nationalityVec', seed=123)\n",
    "    model = word2vec.fit(df)\n",
    "    df = model.transform(df)\n",
    "    df = df.drop('artistNationality')\n",
    "    df = df.drop('nationalityWords')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exact same process as vectorizeNationality\n",
    "def vectorizeObject(df):\n",
    "    objectNameTokenizer = Tokenizer(outputCol=\"objectNameWords\")\n",
    "    objectNameTokenizer.setInputCol(\"objectName\")\n",
    "    df = objectNameTokenizer.transform(df)\n",
    "\n",
    "    word2vecObject = Word2Vec(vectorSize=5, inputCol=\"objectNameWords\", outputCol='objectNameVec', seed=456)\n",
    "    model = word2vecObject.fit(df)\n",
    "    df = model.transform(df)\n",
    "    \n",
    "    df = df.drop('objectName')\n",
    "    df = df.drop('objectNameWords')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotDepartment(df):\n",
    "    stringid = StringIndexer(inputCol='department', outputCol='departmentIndex')\n",
    "    model = stringid.fit(df)\n",
    "    df = model.transform(df)\n",
    "\n",
    "    ohe = OneHotEncoder(inputCol='departmentIndex', outputCol='departmentCode')\n",
    "    model2 = ohe.fit(df)\n",
    "    df = model2.transform(df)\n",
    "    \n",
    "    df = df.drop('department')\n",
    "    df = df.drop('departmentIndex')\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 20:56:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/19 20:56:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets, and transform each one\n",
    "# standard to put 75% of data in training set and 25% in test set\n",
    "transformed_df = transformDataset(df)\n",
    "trainDf, testDf = transformed_df.randomSplit(weights=[0.75, 0.25], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms the dataset into the form we need to train the model\n",
    "def transformIntoInput(df, weightVals:dict=None):\n",
    "    features = df.columns\n",
    "    features.remove('isHighlightInt')\n",
    "    vector = VectorAssembler(inputCols=features, outputCol='features', handleInvalid='skip')\n",
    "    vec = vector.transform(df)\n",
    "    vec = vec.select(col(\"isHighlightInt\").alias(\"label\"), col(\"features\"))\n",
    "\n",
    "    if weightVals != None:\n",
    "        vec = vec.withColumn('weight', when(vec.label == 1, weightVals[1]).otherwise(weightVals[0]))\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the logistic regression model and datasets\n",
    "# uses cross validation and evaluates the models produced to find the best model\n",
    "# by default uses 2/3 of the data in the training set and 1/3 in the validation set\n",
    "def makeAndEvaluateModel(df, regularizationParam = 0., threshold=0.5):\n",
    "    inputVec = transformIntoInput(df)\n",
    "    \n",
    "    logReg = LogisticRegression()\n",
    "    logReg =logReg.setMaxIter(10)\n",
    "    logReg =logReg.setRegParam(regularizationParam)\n",
    "    logReg =logReg.setFeaturesCol('features')\n",
    "    logReg =logReg.setProbabilityCol('probability')\n",
    "    logReg = logReg.setThreshold(threshold)\n",
    "\n",
    "\n",
    "    grid = ParamGridBuilder().addGrid(logReg.maxIter, [0,1]).build()\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=logReg, estimatorParamMaps=grid, evaluator=evaluator, parallelism=2)\n",
    "    cvModel = cv.fit(inputVec)\n",
    "\n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 20:56:34 WARN BlockManager: Block rdd_75_0 already exists on this machine; not re-adding it\n"
     ]
    }
   ],
   "source": [
    "# Train the above model using our training set\n",
    "m = makeAndEvaluateModel(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the metrics of the best model made through cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9973835688121402\n",
      "precision by label:  [0.9973835688121402, 0.0]\n",
      "recall by label:  [1.0, 0.0]\n",
      "F score by label:  [0.9986900707361802, 0.0]\n",
      "area under ROC:  0.9164742917103897\n"
     ]
    }
   ],
   "source": [
    "best = m.bestModel\n",
    "\n",
    "summary = best.summary\n",
    "print(\"Accuracy: \",summary.accuracy)\n",
    "print(\"precision by label: \",summary.precisionByLabel)\n",
    "print(\"recall by label: \", summary.recallByLabel)\n",
    "print(\"F score by label: \", summary.fMeasureByLabel())\n",
    "print(\"area under ROC: \", summary.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial model has a very low F score and recall for the positive classification. We can experiment with the threshold to find the optimal F score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to find optimal f score for positive classifications\n",
    "# Can do this by iterating through a range of thresholds\n",
    "# and finding the highest f score\n",
    "# This takes a while so you can change the step value to make it go\n",
    "# faster\n",
    "thresholds = np.arange(start=0.01, stop=1, step=0.05)\n",
    "precision = []\n",
    "recall = []\n",
    "f = []\n",
    "AUC = []\n",
    "for t in thresholds:\n",
    "    m = makeAndEvaluateModel(trainDf, 0.01, t)\n",
    "    best = m.bestModel\n",
    "    summary = best.summary\n",
    "    precision.append(summary.precisionByLabel[1])\n",
    "    recall.append(summary.recallByLabel[1])\n",
    "    f.append(summary.fMeasureByLabel())\n",
    "    AUC.append(summary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows improvement in the F score for positive classifications, but the recall and precision are still quite low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.060000000000000005\n",
      "\n",
      "highest F score: 0.3\n",
      "precision: 0.3\n",
      "recall:0.3\n",
      "Area under ROC: 0.9164742917103897\n"
     ]
    }
   ],
   "source": [
    "# getting the highest F score that was found along with corresponding precision, recall, and threshold\n",
    "positiveFScores = [x[1] for x in f]\n",
    "maxF = max(positiveFScores)\n",
    "index = positiveFScores.index(maxF)\n",
    "t = thresholds[index]\n",
    "p = precision[index]\n",
    "r = recall[index]\n",
    "auc = AUC[index]\n",
    "print(\"threshold: {d}\\n\\nhighest F score: {a}\\nprecision: {b}\\nrecall:{c}\\nArea under ROC: {e}\".format(a = maxF, b=p, c=r, d=t, e=auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the confusion matrices for the train and test datasets to further evaluate the model selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = makeAndEvaluateModel(trainDf, 0.01, t)\n",
    "trainModelVector = transformIntoInput(trainDf)\n",
    "testModelVector = transformIntoInput(testDf)\n",
    "trainModelVector = bestModel.transform(trainModelVector)\n",
    "testModelVector = bestModel.transform(testModelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrix(modelVector):\n",
    "    # get positive predictions\n",
    "      positivePredictions = modelVector.filter(modelVector.prediction == 1.)\n",
    "      #true positives and false positives\n",
    "      truePositives = positivePredictions.filter(positivePredictions.label == 1.).count()\n",
    "      falsePositives = positivePredictions.filter(positivePredictions.label == 0.).count()\n",
    "      # get all negative predictions\n",
    "      negativePredictions = modelVector.filter(modelVector.prediction == 0.)\n",
    "      # true negatives and false negatives\n",
    "      trueNegatives = negativePredictions.filter(negativePredictions.label == 0.).count()\n",
    "      falseNegatives = negativePredictions.filter(negativePredictions.label == 1.).count()\n",
    "      # print the confusion matrix\n",
    "      print(\"Confusion Matrix:\\n\\\n",
    "            \\t\\t\\tpositive prediction\\tnegative prediction\\n\\\n",
    "            positive label\\t\\t\\t{tp}\\t{fn}\\n\\\n",
    "            negative label\\t\\t\\t{fp}\\t{tn}\"\\\n",
    "            .format(tp = truePositives, fn = falseNegatives, fp = falsePositives, tn = trueNegatives))\n",
    "      return [[truePositives, falseNegatives], [falsePositives, trueNegatives]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix for training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t3\t7\n",
      "            negative label\t\t\t7\t3805\n"
     ]
    }
   ],
   "source": [
    "matrix1 = makeConfusionMatrix(trainModelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t0\t4\n",
      "            negative label\t\t\t2\t1298\n"
     ]
    }
   ],
   "source": [
    "matrix2 = makeConfusionMatrix(testModelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that across both the train and test sets give a poor true positive rate.\n",
    "\n",
    "In this dataset, I think that finding true positives is more important, so I will try to improve the true positive rate.\n",
    "\n",
    "Another way of addressing class imbalance is to add a weight associated with each class. One way to do this is to make the inverse of the proportion of the class the weight like so: $$\\begin{align}\n",
    "p_{positive} &= \\frac{\\Sigma^n_{i=0}{y_i}}{n},\\ y\\in \\{0,1\\} \\\\\n",
    "W_{positive} &= \\frac{1}{p_{positive}} \\\\\n",
    "p_{negative} &= 1-p_{positive} \\\\\n",
    "W_{negative} &= \\frac{1}{p_{negative}} \\\\\n",
    "\\end{align}$$\n",
    "Where $y$ is the label column in the dataset. This will give more importance to the minority class in the dataset.\n",
    "The weights for this dataset are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366.2857142857143\n",
      "1.0027375831052014\n"
     ]
    }
   ],
   "source": [
    "df = transformDataset(df)\n",
    "totalHighlights = df.select('isHighlightInt').rdd.map(lambda x: x[0]).sum()\n",
    "propHighlight = totalHighlights/df.count()\n",
    "weightHighlight = 1/propHighlight\n",
    "print(weightHighlight)\n",
    "propNonHighlight = 1 - propHighlight\n",
    "weightNonHighlight = 1/propNonHighlight\n",
    "print(weightNonHighlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new model that implements the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModelWithWeights(df, regularizationParam = 0., threshold=0.5):\n",
    "    input_vec = transformIntoInput(df, weightVals={0: weightNonHighlight, 1: weightHighlight})\n",
    "    logReg = LogisticRegression()\n",
    "    logReg = logReg.setMaxIter(10)\n",
    "    logReg = logReg.setRegParam(regularizationParam)\n",
    "    logReg = logReg.setFeaturesCol('features')\n",
    "    logReg = logReg.setProbabilityCol('probability')\n",
    "    logReg = logReg.setWeightCol('weight')\n",
    "    logReg = logReg.setThreshold(threshold)\n",
    "\n",
    "    grid = ParamGridBuilder().addGrid(logReg.maxIter, [0,1]).build()\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    cv = CrossValidator(estimator=logReg, estimatorParamMaps=grid, evaluator=evaluator, parallelism=2)\n",
    "    cvModel = cv.fit(input_vec)\n",
    "\n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mWeighted = makeModelWithWeights(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7606690396907412\n",
      "precision by label:  [0.7904613619601705, 0.734559034760129]\n",
      "recall by label:  [0.7229800629590752, 0.8]\n",
      "F score by label:  [0.7552162849872751, 0.7658841588977512]\n",
      "area under ROC:  0.8675760755508924\n"
     ]
    }
   ],
   "source": [
    "bestWeighted = mWeighted.bestModel\n",
    "\n",
    "weightedSummary = bestWeighted.summary\n",
    "print(\"Accuracy: \",weightedSummary.accuracy)\n",
    "print(\"precision by label: \",weightedSummary.precisionByLabel)\n",
    "print(\"recall by label: \", weightedSummary.recallByLabel)\n",
    "print(\"F score by label: \", weightedSummary.fMeasureByLabel())\n",
    "print(\"area under ROC: \", weightedSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model improves the recall and F score drastically, while keeping precision and accuracy fairly high.\n",
    "We can find the confusion matrices of this model for the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedTrainModelVector = transformIntoInput(trainDf, {0: weightNonHighlight, 1: weightHighlight})\n",
    "weightedTestModelVector = transformIntoInput(testDf, {0: weightNonHighlight, 1: weightHighlight})\n",
    "weightedTrainModelVector = bestWeighted.transform(weightedTrainModelVector)\n",
    "weightedTestModelVector = bestWeighted.transform(weightedTestModelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t8\t2\n",
      "            negative label\t\t\t1056\t2756\n"
     ]
    }
   ],
   "source": [
    "wMatrix1 = makeConfusionMatrix(weightedTrainModelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t2\t2\n",
      "            negative label\t\t\t399\t901\n"
     ]
    }
   ],
   "source": [
    "wMatrix2 = makeConfusionMatrix(weightedTestModelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model improved the true positive rate, but now the rate of false positives is really high. We can try to adjust the threshold like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.7100000000000001\n",
      "\n",
      "highest F score: 0.78678652278072\n",
      "precision: 0.8981383912890755\n",
      "recall:0.7\n",
      "Area under ROC: 0.8675760755508924\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(start=0.01, stop=1, step=0.05)\n",
    "precision = []\n",
    "recall = []\n",
    "f = []\n",
    "AUC = []\n",
    "models = []\n",
    "for t in thresholds:\n",
    "    m = makeModelWithWeights(trainDf, 0.01, t)\n",
    "    best = m.bestModel\n",
    "    summary = best.summary\n",
    "    precision.append(summary.precisionByLabel[1])\n",
    "    recall.append(summary.recallByLabel[1])\n",
    "    f.append(summary.fMeasureByLabel())\n",
    "    AUC.append(summary.areaUnderROC)\n",
    "    models.append(best)\n",
    "\n",
    "# getting the highest F score that was found along with corresponding precision, recall, and threshold\n",
    "positiveFScores = [x[1] for x in f]\n",
    "maxF = max(positiveFScores)\n",
    "index = positiveFScores.index(maxF)\n",
    "t = thresholds[index]\n",
    "p = precision[index]\n",
    "r = recall[index]\n",
    "auc = AUC[index]\n",
    "print(\"threshold: {d}\\n\\nhighest F score: {a}\\nprecision: {b}\\nrecall:{c}\\nArea under ROC: {e}\".format(a = maxF, b=p, c=r, d=t, e=auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a model using this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalModel = models[index]\n",
    "trainModelVector = transformIntoInput(trainDf)\n",
    "testModelVector = transformIntoInput(testDf)\n",
    "trainModelVector = finalModel.transform(trainModelVector)\n",
    "testModelVector = finalModel.transform(testModelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8143491407356328\n",
      "precision by label:  [0.762691415313216, 0.8981383912890755]\n",
      "recall by label:  [0.9239244491080757, 0.7]\n",
      "F score by label:  [0.8356013489467639, 0.78678652278072]\n",
      "area under ROC:  0.8675760755508924\n"
     ]
    }
   ],
   "source": [
    "finalSummary = finalModel.summary\n",
    "print(\"Accuracy: \",finalSummary.accuracy)\n",
    "print(\"precision by label: \",finalSummary.precisionByLabel)\n",
    "print(\"recall by label: \", finalSummary.recallByLabel)\n",
    "print(\"F score by label: \", finalSummary.fMeasureByLabel())\n",
    "print(\"area under ROC: \", finalSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t7\t3\n",
      "            negative label\t\t\t290\t3522\n"
     ]
    }
   ],
   "source": [
    "finalMatrix1 = makeConfusionMatrix(trainModelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            \t\t\tpositive prediction\tnegative prediction\n",
      "            positive label\t\t\t2\t2\n",
      "            negative label\t\t\t121\t1179\n"
     ]
    }
   ],
   "source": [
    "finalMatrix1 = makeConfusionMatrix(testModelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true positive rate hasn't changed much, but the rate of false positives has improved. This is the final model I will go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "modelPath = os.getcwd() + \"/final_model\"\n",
    "finalModel.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
